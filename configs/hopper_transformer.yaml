# transformer_config.yaml
npz: /public/sparse_d4rl/mapped/concat/hopper_one_step_train.npz
out: /data/sparse_d4rl/pretrained/ddim_transformer_hopper_medium_v2

# training
epochs: 200
batch: 512
lr: 0.0002
wd: 0.0
timesteps: 1000
checkpoint_every_epochs: 20
seed: 0
device: cuda

# model
model: transformer
d_model: 256
nhead: 8
tf_layers: 4
ff_dim: 512
time_emb: 128
dropout: 0.1

# logging
wandb: true
wandb_project: GORMPO
wandb_entity: jhhuang_chloe
wandb_run: ddim_transformer_hopper_medium_v2
log_every: 100
samples_every: 1