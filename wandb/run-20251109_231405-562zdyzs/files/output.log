setting device: cuda:5
Environment without noise
opened the pickle file for synthetic dataset
Model loaded from: /public/gormpo/models/walker2d/realnvp_model.pth
Metadata loaded from: /public/gormpo/models/walker2d/realnvp_meta_data.pkl
Threshold: -26.542936325073242
training log mean and std of classifier:  17.94239044189453 14.978002548217773
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
[1;32m [info][0m: Start training dynamics
[1;32m [info][0m: loss/model_eval_mse_loss: 2.637552
[1;32m [info][0m: loss/model_eval_mse_loss: 0.615917
Traceback (most recent call last):
  File "mopo.py", line 182, in <module>
    main(args=get_args())
  File "mopo.py", line 162, in main
    policy, trainer = train(env, run, logger, seed, args)
  File "/home/ubuntu/GORMPO/train.py", line 183, in train
    trainer.train_dynamics()
  File "/home/ubuntu/GORMPO/trainer.py", line 99, in train_dynamics
    self.algo.learn_dynamics()
  File "/home/ubuntu/GORMPO/algo/mopo.py", line 105, in learn_dynamics
    model_log_infos = self.dynamics_model.update(train_data_batch)
  File "/home/ubuntu/GORMPO/transition_model.py", line 139, in update
    groundtruths = torch.cat((delta_obs_batch, reward_batch), dim=-1).to(util.device)
KeyboardInterrupt
Traceback (most recent call last):
  File "mopo.py", line 182, in <module>
    main(args=get_args())
  File "mopo.py", line 162, in main
    policy, trainer = train(env, run, logger, seed, args)
  File "/home/ubuntu/GORMPO/train.py", line 183, in train
    trainer.train_dynamics()
  File "/home/ubuntu/GORMPO/trainer.py", line 99, in train_dynamics
    self.algo.learn_dynamics()
  File "/home/ubuntu/GORMPO/algo/mopo.py", line 105, in learn_dynamics
    model_log_infos = self.dynamics_model.update(train_data_batch)
  File "/home/ubuntu/GORMPO/transition_model.py", line 139, in update
    groundtruths = torch.cat((delta_obs_batch, reward_batch), dim=-1).to(util.device)
KeyboardInterrupt
