setting device: cuda:5
Environment without noise
opened the pickle file for synthetic dataset
Model loaded from: /public/gormpo/models/walker2d/realnvp_model.pth
Metadata loaded from: /public/gormpo/models/walker2d/realnvp_meta_data.pkl
Threshold: -26.542936325073242
training log mean and std of classifier:  17.94239044189453 14.978002548217773
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
[1;32m [info][0m: Start training dynamics
[1;32m [info][0m: loss/model_eval_mse_loss: 2.637552
[1;32m [info][0m: loss/model_eval_mse_loss: 0.615917
[1;32m [info][0m: loss/model_eval_mse_loss: 0.512116
[1;32m [info][0m: loss/model_eval_mse_loss: 0.468983
[1;32m [info][0m: loss/model_eval_mse_loss: 0.437512
[1;32m [info][0m: loss/model_eval_mse_loss: 0.410507
[1;32m [info][0m: loss/model_eval_mse_loss: 0.387698
[1;32m [info][0m: loss/model_eval_mse_loss: 0.374601
[1;32m [info][0m: loss/model_eval_mse_loss: 0.359176
[1;32m [info][0m: loss/model_eval_mse_loss: 0.342875
[1;32m [info][0m: loss/model_eval_mse_loss: 0.327921
[1;32m [info][0m: loss/model_eval_mse_loss: 0.327674
[1;32m [info][0m: loss/model_eval_mse_loss: 0.316809
[1;32m [info][0m: loss/model_eval_mse_loss: 0.314163
[1;32m [info][0m: loss/model_eval_mse_loss: 0.303014
[1;32m [info][0m: loss/model_eval_mse_loss: 0.304803
[1;32m [info][0m: loss/model_eval_mse_loss: 0.302406
[1;32m [info][0m: loss/model_eval_mse_loss: 0.300513
[1;32m [info][0m: loss/model_eval_mse_loss: 0.299695
[1;32m [info][0m: loss/model_eval_mse_loss: 0.295499
[1;32m [info][0m: loss/model_eval_mse_loss: 0.294187
[1;32m [info][0m: loss/model_eval_mse_loss: 0.289566
[1;32m [info][0m: loss/model_eval_mse_loss: 0.289540
[1;32m [info][0m: loss/model_eval_mse_loss: 0.289725
[1;32m [info][0m: loss/model_eval_mse_loss: 0.289299
[1;32m [info][0m: loss/model_eval_mse_loss: 0.284221
[1;32m [info][0m: loss/model_eval_mse_loss: 0.284102
[1;32m [info][0m: loss/model_eval_mse_loss: 0.284531
[1;32m [info][0m: loss/model_eval_mse_loss: 0.279220
[1;32m [info][0m: loss/model_eval_mse_loss: 0.275757
[1;32m [info][0m: loss/model_eval_mse_loss: 0.272891
[1;32m [info][0m: loss/model_eval_mse_loss: 0.270290
[1;32m [info][0m: loss/model_eval_mse_loss: 0.268951
[1;32m [info][0m: loss/model_eval_mse_loss: 0.264724
[1;32m [info][0m: loss/model_eval_mse_loss: 0.261001
[1;32m [info][0m: loss/model_eval_mse_loss: 0.255753
[1;32m [info][0m: loss/model_eval_mse_loss: 0.251236
[1;32m [info][0m: loss/model_eval_mse_loss: 0.248005
[1;32m [info][0m: loss/model_eval_mse_loss: 0.250096
[1;32m [info][0m: loss/model_eval_mse_loss: 0.240325
[1;32m [info][0m: loss/model_eval_mse_loss: 0.242008
[1;32m [info][0m: loss/model_eval_mse_loss: 0.240344
[1;32m [info][0m: loss/model_eval_mse_loss: 0.228418
[1;32m [info][0m: loss/model_eval_mse_loss: 0.226609
[1;32m [info][0m: loss/model_eval_mse_loss: 0.227639
[1;32m [info][0m: loss/model_eval_mse_loss: 0.219551
[1;32m [info][0m: loss/model_eval_mse_loss: 0.218797
[1;32m [info][0m: loss/model_eval_mse_loss: 0.209116
[1;32m [info][0m: loss/model_eval_mse_loss: 0.212114
[1;32m [info][0m: loss/model_eval_mse_loss: 0.204940
[1;32m [info][0m: loss/model_eval_mse_loss: 0.205120
[1;32m [info][0m: loss/model_eval_mse_loss: 0.196253
[1;32m [info][0m: loss/model_eval_mse_loss: 0.201724
[1;32m [info][0m: loss/model_eval_mse_loss: 0.190866
[1;32m [info][0m: loss/model_eval_mse_loss: 0.185150
[1;32m [info][0m: loss/model_eval_mse_loss: 0.182617
[1;32m [info][0m: loss/model_eval_mse_loss: 0.180611
[1;32m [info][0m: loss/model_eval_mse_loss: 0.181938
[1;32m [info][0m: loss/model_eval_mse_loss: 0.171314
[1;32m [info][0m: loss/model_eval_mse_loss: 0.167109
[1;32m [info][0m: loss/model_eval_mse_loss: 0.165245
[1;32m [info][0m: loss/model_eval_mse_loss: 0.160428
[1;32m [info][0m: loss/model_eval_mse_loss: 0.159433
[1;32m [info][0m: loss/model_eval_mse_loss: 0.154628
[1;32m [info][0m: loss/model_eval_mse_loss: 0.154161
[1;32m [info][0m: loss/model_eval_mse_loss: 0.153835
[1;32m [info][0m: loss/model_eval_mse_loss: 0.155336
[1;32m [info][0m: loss/model_eval_mse_loss: 0.147765
[1;32m [info][0m: loss/model_eval_mse_loss: 0.158368
[1;32m [info][0m: loss/model_eval_mse_loss: 0.144843
[1;32m [info][0m: loss/model_eval_mse_loss: 0.139972
[1;32m [info][0m: loss/model_eval_mse_loss: 0.139749
[1;32m [info][0m: loss/model_eval_mse_loss: 0.139591
[1;32m [info][0m: loss/model_eval_mse_loss: 0.141641
[1;32m [info][0m: loss/model_eval_mse_loss: 0.137154
[1;32m [info][0m: loss/model_eval_mse_loss: 0.130716
[1;32m [info][0m: loss/model_eval_mse_loss: 0.135181
[1;32m [info][0m: loss/model_eval_mse_loss: 0.142442
[1;32m [info][0m: loss/model_eval_mse_loss: 0.140332
[1;32m [info][0m: loss/model_eval_mse_loss: 0.131292
[1;32m [info][0m: loss/model_eval_mse_loss: 0.128305
[1;32m [info][0m: loss/model_eval_mse_loss: 0.127775
[1;32m [info][0m: loss/model_eval_mse_loss: 0.126816
[1;32m [info][0m: loss/model_eval_mse_loss: 0.125729
[1;32m [info][0m: loss/model_eval_mse_loss: 0.122839
[1;32m [info][0m: loss/model_eval_mse_loss: 0.142655
[1;32m [info][0m: loss/model_eval_mse_loss: 0.126751
[1;32m [info][0m: loss/model_eval_mse_loss: 0.121119
[1;32m [info][0m: loss/model_eval_mse_loss: 0.121011
[1;32m [info][0m: loss/model_eval_mse_loss: 0.125635
[1;32m [info][0m: loss/model_eval_mse_loss: 0.121150
[1;32m [info][0m: loss/model_eval_mse_loss: 0.119933
[1;32m [info][0m: loss/model_eval_mse_loss: 0.120908
[1;32m [info][0m: loss/model_eval_mse_loss: 0.121267
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118271
[1;32m [info][0m: loss/model_eval_mse_loss: 0.122903
[1;32m [info][0m: loss/model_eval_mse_loss: 0.117017
[1;32m [info][0m: loss/model_eval_mse_loss: 0.117846
[1;32m [info][0m: loss/model_eval_mse_loss: 0.122811
[1;32m [info][0m: loss/model_eval_mse_loss: 0.119246
[1;32m [info][0m: loss/model_eval_mse_loss: 0.117385
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118110
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118557
[1;32m [info][0m: loss/model_eval_mse_loss: 0.120773
[1;32m [info][0m: loss/model_eval_mse_loss: 0.115096
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118588
[1;32m [info][0m: loss/model_eval_mse_loss: 0.113577
[1;32m [info][0m: loss/model_eval_mse_loss: 0.125870
[1;32m [info][0m: loss/model_eval_mse_loss: 0.113282
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118350
[1;32m [info][0m: loss/model_eval_mse_loss: 0.114018
[1;32m [info][0m: loss/model_eval_mse_loss: 0.117873
[1;32m [info][0m: loss/model_eval_mse_loss: 0.115122
[1;32m [info][0m: loss/model_eval_mse_loss: 0.115778
[1;32m [info][0m: loss/model_eval_mse_loss: 0.121467
[1;32m [info][0m: loss/model_eval_mse_loss: 0.120158
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118382
[1;32m [info][0m: loss/model_eval_mse_loss: 0.112765
[1;32m [info][0m: loss/model_eval_mse_loss: 0.118519
[1;32m [info][0m: loss/model_eval_mse_loss: 0.110824
[1;32m [info][0m: loss/model_eval_mse_loss: 0.114741
[1;32m [info][0m: loss/model_eval_mse_loss: 0.112898
[1;32m [info][0m: loss/model_eval_mse_loss: 0.110188
[1;32m [info][0m: loss/model_eval_mse_loss: 0.111126
[1;32m [info][0m: loss/model_eval_mse_loss: 0.117978
[1;32m [info][0m: loss/model_eval_mse_loss: 0.124460
[1;32m [info][0m: loss/model_eval_mse_loss: 0.120409
[1;32m [info][0m: loss/model_eval_mse_loss: 0.150037
[1;32m [info][0m: total time: 6941.206s
Epoch #1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 52.63it/s, alpha=0.74, entropy=4.12, loss/actor=-3.29, loss/alpha=-3.04, loss/critic1=37.7, loss/critic2=0.176]
Epoch #2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 51.79it/s, alpha=0.548, entropy=4.08, loss/actor=-2.47, loss/alpha=-6.05, loss/critic1=28.7, loss/critic2=0.0895]
Epoch #3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 52.44it/s, alpha=0.406, entropy=4.1, loss/actor=-1.89, loss/alpha=-9.1, loss/critic1=24.4, loss/critic2=0.0846]
Epoch #4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:18<00:00, 54.07it/s, alpha=0.301, entropy=4.13, loss/actor=-1.46, loss/alpha=-12.2, loss/critic1=19.6, loss/critic2=0.0409]
Epoch #5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 51.06it/s, alpha=0.223, entropy=4.08, loss/actor=-1.15, loss/alpha=-15.1, loss/critic1=16.8, loss/critic2=0.172]
Epoch #6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 52.47it/s, alpha=0.165, entropy=4.07, loss/actor=-0.895, loss/alpha=-18.1, loss/critic1=15.5, loss/critic2=0.0295]
Epoch #7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:20<00:00, 49.85it/s, alpha=0.122, entropy=4.08, loss/actor=-0.729, loss/alpha=-21.2, loss/critic1=13.9, loss/critic2=0.0257]
Epoch #8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 50.23it/s, alpha=0.0907, entropy=4.03, loss/actor=-0.622, loss/alpha=-24.1, loss/critic1=13.1, loss/critic2=0.0442]
Epoch #9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 50.37it/s, alpha=0.0672, entropy=4.02, loss/actor=-0.504, loss/alpha=-27.1, loss/critic1=12.1, loss/critic2=0.0255]
Epoch #10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:19<00:00, 51.57it/s, alpha=0.0498, entropy=3.95, loss/actor=-0.434, loss/alpha=-29.8, loss/critic1=12.1, loss/critic2=0.0635]
Traceback (most recent call last):
  File "mopo.py", line 183, in <module>
    main(args=get_args())
  File "mopo.py", line 163, in main
    policy, trainer = train(env, run, logger, seed, args)
  File "/home/ubuntu/GORMPO/train.py", line 189, in train
    trainer.train_policy()
  File "/home/ubuntu/GORMPO/trainer.py", line 137, in train_policy
    eval_info = self._evaluate()
  File "/home/ubuntu/GORMPO/trainer.py", line 186, in _evaluate
    next_obs, reward, terminal= self.eval_env.step(action) #next_obs = world model forecast
ValueError: too many values to unpack (expected 3)
Traceback (most recent call last):
  File "mopo.py", line 183, in <module>
    main(args=get_args())
  File "mopo.py", line 163, in main
    policy, trainer = train(env, run, logger, seed, args)
  File "/home/ubuntu/GORMPO/train.py", line 189, in train
    trainer.train_policy()
  File "/home/ubuntu/GORMPO/trainer.py", line 137, in train_policy
    eval_info = self._evaluate()
  File "/home/ubuntu/GORMPO/trainer.py", line 186, in _evaluate
    next_obs, reward, terminal= self.eval_env.step(action) #next_obs = world model forecast
ValueError: too many values to unpack (expected 3)
